{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SaRDinE Training\n",
                "\n",
                "**Fully self-contained notebook - writes and launches training script**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q transformers==5.0.0rc1 torch datasets accelerate wandb huggingface_hub bitsandbytes tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "if not os.path.exists('srde.py'):\n",
                "    print(\"Cloning SRDE repo...\")\n",
                "    !git clone https://github.com/MinimaML/srde-mistral\n",
                "    %cd srde-mistral/mistral-3-14b-reasoning\n",
                "else:\n",
                "    print(\"SRDE found.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPUs\n",
                "!nvidia-smi -L\n",
                "import subprocess\n",
                "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)\n",
                "NUM_GPUS = len([l for l in result.stdout.strip().split('\\n') if l.startswith('GPU')])\n",
                "print(f\"\\nDetected {NUM_GPUS} GPUs\")\n",
                "# Clear any existing stop file\n",
                "!rm -f STOP_TRAINING"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === WRITE TRAINING SCRIPT ===\n",
                "\n",
                "TRAIN_SCRIPT = r'''\n",
                "#!/usr/bin/env python3\n",
                "\"\"\"SaRDinE Multi-GPU Training Script\n",
                "\n",
                "Launch with: accelerate launch --num_processes=N --mixed_precision=bf16 train.py\n",
                "\"\"\"\n",
                "import os, gc, random, threading, time\n",
                "from collections import deque\n",
                "from pathlib import Path\n",
                "\n",
                "import torch\n",
                "import torch.distributed as dist\n",
                "import wandb\n",
                "import bitsandbytes as bnb\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, get_cosine_schedule_with_warmup\n",
                "from torch.utils.data import DataLoader, IterableDataset\n",
                "from tqdm.auto import tqdm\n",
                "from huggingface_hub import login\n",
                "from accelerate import Accelerator\n",
                "\n",
                "from config import SRDEConfig\n",
                "from srde import create_srde_model\n",
                "from muon import Muon\n",
                "\n",
                "os.environ['WANDB_API_KEY'] = 'abe32d5463fb2265eaea4563a571c07e5a39b7b6'\n",
                "login(token='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')\n",
                "\n",
                "MODEL_NAME = 'mistralai/Ministral-3-14B-Reasoning-2512'\n",
                "SRDE_CFG = SRDEConfig()\n",
                "CONFIG = {\n",
                "    'wandb_project': 'sardine-collab', 'model_name': MODEL_NAME, 'max_length': 2048,\n",
                "    'target_tokens': 20_000_000_000, 'buffer_tokens': 1_000_000, 'max_steps': 1000000,\n",
                "    'lr': 1e-4, 'muon_lr': 0.02, 'warmup_steps': 1000, 'save_steps': 5000,\n",
                "    'batch_size': 8, 'grad_accum': 4, 'checkpoint_dir': './checkpoints',\n",
                "}\n",
                "\n",
                "DOMAINS = {\n",
                "    'math': {'expert': 0, 'weight': 0.30}, 'code': {'expert': 1, 'weight': 0.30},\n",
                "    'science': {'expert': 2, 'weight': 0.15}, 'logic': {'expert': 3, 'weight': 0.10},\n",
                "    'planning': {'expert': 4, 'weight': 0.08}, 'abstract': {'expert': 5, 'weight': 0.07},\n",
                "}\n",
                "\n",
                "DATA_SOURCES = {\n",
                "    'math': [('nvidia/OpenMathInstruct-2', None), ('meta-math/MetaMathQA', None)],\n",
                "    'code': [('ise-uiuc/Magicoder-OSS-Instruct-75K', None), ('bigcode/starcoderdata', None)],\n",
                "    'science': [('HuggingFaceFW/fineweb-edu', 'sample-10BT')],\n",
                "    'logic': [('Rowan/hellaswag', None)],\n",
                "    'planning': [('hotpot_qa', 'fullwiki')],\n",
                "    'abstract': [('deepmind/aqua_rat', 'raw'), ('winogrande', 'winogrande_xl')],\n",
                "}\n",
                "\n",
                "def format_sample(sample, source_name):\n",
                "    try:\n",
                "        if 'OpenMath' in source_name:\n",
                "            return f\"Problem: {sample.get('problem', '')}\\nSolution: {sample.get('generated_solution', '')}\"\n",
                "        if 'MetaMath' in source_name:\n",
                "            return f\"Q: {sample.get('query', '')}\\nA: {sample.get('response', '')}\"\n",
                "        if 'Magicoder' in source_name:\n",
                "            return f\"### Instruction:\\n{sample.get('problem', '')}\\n### Solution:\\n{sample.get('solution', '')}\"\n",
                "        if 'starcoder' in source_name:\n",
                "            return sample.get('content', '')\n",
                "        if 'fineweb' in source_name:\n",
                "            return sample.get('text', '')\n",
                "        if 'hellaswag' in source_name:\n",
                "            return f\"{sample.get('ctx', '')} {sample.get('endings', [''])[int(sample.get('label', 0))]}\"\n",
                "        if 'hotpot' in source_name:\n",
                "            return f\"Q: {sample.get('question', '')}\\nA: {sample.get('answer', '')}\"\n",
                "        if 'aqua' in source_name:\n",
                "            return f\"Q: {sample.get('question', '')}\\nA: {sample.get('rationale', '')}\"\n",
                "        if 'winogrande' in source_name:\n",
                "            return sample.get('sentence', '')\n",
                "        return sample.get('text', str(sample)[:1000])\n",
                "    except:\n",
                "        return ''\n",
                "\n",
                "class BufferedDomainStream:\n",
                "    def __init__(self, domain, sources, tokenizer, buffer_tokens=1_000_000):\n",
                "        self.domain, self.sources, self.tokenizer = domain, sources, tokenizer\n",
                "        self.buffer_tokens, self.buffer, self.buffer_token_count = buffer_tokens, deque(), 0\n",
                "        self.lock, self.stop_event, self.thread = threading.Lock(), threading.Event(), None\n",
                "    def start(self):\n",
                "        self.thread = threading.Thread(target=self._fill_loop, daemon=True)\n",
                "        self.thread.start()\n",
                "    def stop(self):\n",
                "        self.stop_event.set()\n",
                "        if self.thread: self.thread.join(timeout=1)\n",
                "    def _fill_loop(self):\n",
                "        while not self.stop_event.is_set():\n",
                "            with self.lock:\n",
                "                if self.buffer_token_count >= self.buffer_tokens:\n",
                "                    time.sleep(0.1)\n",
                "                    continue\n",
                "            for src, sub in self.sources:\n",
                "                if self.stop_event.is_set(): break\n",
                "                try:\n",
                "                    ds = load_dataset(src, sub, split='train', streaming=True, token=True)\n",
                "                    for s in ds:\n",
                "                        if self.stop_event.is_set(): break\n",
                "                        txt = format_sample(s, src)\n",
                "                        if len(txt) < 50: continue\n",
                "                        toks = len(self.tokenizer.encode(txt, add_special_tokens=False))\n",
                "                        with self.lock:\n",
                "                            self.buffer.append({'text': txt, 'domain': self.domain, 'tokens': toks})\n",
                "                            self.buffer_token_count += toks\n",
                "                            if self.buffer_token_count >= self.buffer_tokens * 1.2: break\n",
                "                except Exception as e:\n",
                "                    print(f'[{self.domain}] Stream error {src}: {e}')\n",
                "                    time.sleep(2)\n",
                "    def get_sample(self):\n",
                "        with self.lock:\n",
                "            if self.buffer:\n",
                "                s = self.buffer.popleft()\n",
                "                self.buffer_token_count -= s['tokens']\n",
                "                return s\n",
                "        return None\n",
                "    def buffer_status(self):\n",
                "        with self.lock: return self.buffer_token_count\n",
                "\n",
                "class BufferedStreamDataset(IterableDataset):\n",
                "    def __init__(self, domains, tokenizer, buffer_tokens=1_000_000, rank=0):\n",
                "        self.domains = list(domains.keys())\n",
                "        self.weights = [domains[d]['weight'] for d in self.domains]\n",
                "        self.tokenizer, self.buffer_tokens = tokenizer, buffer_tokens\n",
                "        self.streams, self.started = {}, False\n",
                "        self.rank = rank\n",
                "    def start_streams(self):\n",
                "        if self.started: return\n",
                "        if self.rank == 0: print('Starting streams...')\n",
                "        for d in self.domains:\n",
                "            self.streams[d] = BufferedDomainStream(d, DATA_SOURCES.get(d, []), self.tokenizer, self.buffer_tokens)\n",
                "            self.streams[d].start()\n",
                "        self.started = True\n",
                "        while True:\n",
                "            m = min(s.buffer_status() for s in self.streams.values())\n",
                "            if self.rank == 0: print(f'  Buffer: {m/1e6:.2f}M/{self.buffer_tokens/1e6:.0f}M', end='\\r')\n",
                "            if m >= self.buffer_tokens * 0.5:\n",
                "                if self.rank == 0: print('\\nReady!')\n",
                "                break\n",
                "            time.sleep(1)\n",
                "    def stop_streams(self):\n",
                "        for s in self.streams.values(): s.stop()\n",
                "    def __iter__(self):\n",
                "        self.start_streams()\n",
                "        rng = random.Random(42 + self.rank)\n",
                "        empty_count = 0\n",
                "        while True:\n",
                "            d = rng.choices(self.domains, weights=self.weights, k=1)[0]\n",
                "            s = self.streams[d].get_sample()\n",
                "            if s:\n",
                "                empty_count = 0\n",
                "                yield s\n",
                "            else:\n",
                "                found = False\n",
                "                for dd in self.domains:\n",
                "                    s = self.streams[dd].get_sample()\n",
                "                    if s:\n",
                "                        empty_count = 0\n",
                "                        found = True\n",
                "                        yield s\n",
                "                        break\n",
                "                if not found:\n",
                "                    empty_count += 1\n",
                "                    if empty_count > 100:\n",
                "                        time.sleep(0.1)\n",
                "                        empty_count = 0\n",
                "\n",
                "def get_loss(out):\n",
                "    return out.get('loss') if isinstance(out, dict) else getattr(out, 'loss', None)\n",
                "\n",
                "def get_params_for_muon(p):\n",
                "    return [x for x in p if x.ndim >= 2], [x for x in p if x.ndim < 2]\n",
                "\n",
                "def get_gpu_stats():\n",
                "    s = {}\n",
                "    if torch.cuda.is_available():\n",
                "        for i in range(torch.cuda.device_count()):\n",
                "            a = torch.cuda.memory_allocated(i) / 1e9\n",
                "            t = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
                "            s[f'gpu{i}_gb'] = a\n",
                "            s[f'gpu{i}_pct'] = (a / t) * 100\n",
                "        s['gpu_total_gb'] = sum(torch.cuda.memory_allocated(i) for i in range(torch.cuda.device_count())) / 1e9\n",
                "    return s\n",
                "\n",
                "def cleanup():\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()\n",
                "    if dist.is_initialized():\n",
                "        dist.destroy_process_group()\n",
                "\n",
                "def main():\n",
                "    # Get local rank from environment (set by accelerate/torchrun)\n",
                "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
                "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
                "    \n",
                "    # Set CUDA device BEFORE any CUDA operations\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.set_device(local_rank)\n",
                "    \n",
                "    print(f'[Rank {local_rank}/{world_size}] Starting on cuda:{local_rank}')\n",
                "    \n",
                "    acc = Accelerator(mixed_precision='bf16', gradient_accumulation_steps=CONFIG['grad_accum'])\n",
                "    is_main, device = acc.is_main_process, acc.device\n",
                "    \n",
                "    print(f'[Rank {local_rank}] Accelerator device: {device}')\n",
                "    \n",
                "    if is_main:\n",
                "        wandb.init(project=CONFIG['wandb_project'], name=f'sardine-{world_size}gpu', config={**CONFIG, 'world_size': world_size})\n",
                "    \n",
                "    tok = AutoTokenizer.from_pretrained(CONFIG['model_name'], trust_remote_code=True)\n",
                "    tok.pad_token = tok.eos_token\n",
                "    \n",
                "    ds = BufferedStreamDataset(DOMAINS, tok, CONFIG['buffer_tokens'], rank=local_rank)\n",
                "    \n",
                "    def collate(batch):\n",
                "        e = tok([b['text'][:8192] for b in batch], truncation=True, max_length=CONFIG['max_length'], padding='max_length', return_tensors='pt')\n",
                "        return e['input_ids'], e['attention_mask']\n",
                "    \n",
                "    dl = DataLoader(ds, batch_size=CONFIG['batch_size'], collate_fn=collate)\n",
                "    \n",
                "    # Load model directly to this rank's GPU\n",
                "    print(f'[Rank {local_rank}] Loading model to cuda:{local_rank}...')\n",
                "    model = create_srde_model(\n",
                "        CONFIG['model_name'], \n",
                "        torch_dtype=torch.bfloat16, \n",
                "        trust_remote_code=True, \n",
                "        device_map={'': local_rank}  # Load directly to this GPU\n",
                "    )\n",
                "    \n",
                "    tc = 0\n",
                "    for n, p in model.named_parameters():\n",
                "        if 'srde_layers' in n or 'expert' in n or 'router' in n or 'vocabulary' in n:\n",
                "            p.requires_grad = True\n",
                "            tc += p.numel()\n",
                "        else:\n",
                "            p.requires_grad = False\n",
                "    print(f'[Rank {local_rank}] Trainable: {tc/1e6:.1f}M')\n",
                "    \n",
                "    if hasattr(model, 'base_model') and hasattr(model.base_model, 'gradient_checkpointing_enable'):\n",
                "        model.base_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})\n",
                "    \n",
                "    all_p = [p for p in model.parameters() if p.requires_grad]\n",
                "    muon_p, adam_p = get_params_for_muon(all_p)\n",
                "    print(f'[Rank {local_rank}] Muon: {sum(p.numel() for p in muon_p)/1e6:.1f}M | Adam: {sum(p.numel() for p in adam_p)/1e6:.1f}M')\n",
                "    \n",
                "    opts, scheds = [], []\n",
                "    if muon_p:\n",
                "        o = Muon(muon_p, lr=CONFIG['muon_lr'], momentum=0.95)\n",
                "        opts.append(o)\n",
                "        scheds.append(get_cosine_schedule_with_warmup(o, CONFIG['warmup_steps'], CONFIG['max_steps']))\n",
                "    if adam_p:\n",
                "        o = bnb.optim.AdamW8bit(adam_p, lr=CONFIG['lr'])\n",
                "        opts.append(o)\n",
                "        scheds.append(get_cosine_schedule_with_warmup(o, CONFIG['warmup_steps'], CONFIG['max_steps']))\n",
                "    \n",
                "    # Prepare with accelerate - wraps with DDP for gradient sync\n",
                "    model, dl, *rest = acc.prepare(model, dl, *opts, *scheds)\n",
                "    n = len(opts)\n",
                "    opts, scheds = list(rest[:n]), list(rest[n:])\n",
                "    \n",
                "    print(f'[Rank {local_rank}] Post-prepare GPU: {get_gpu_stats()}')\n",
                "    \n",
                "    Path(CONFIG['checkpoint_dir']).mkdir(exist_ok=True, parents=True)\n",
                "    step, total_tokens = 0, 0\n",
                "    tpb = CONFIG['batch_size'] * CONFIG['max_length'] * world_size\n",
                "    \n",
                "    pbar = tqdm(total=CONFIG['target_tokens'], unit='tok', unit_scale=True, disable=not is_main)\n",
                "    \n",
                "    model.train()\n",
                "    print(f'[Rank {local_rank}] Starting training loop...')\n",
                "    \n",
                "    try:\n",
                "        for ids, mask in dl:\n",
                "            if total_tokens >= CONFIG['target_tokens']:\n",
                "                print(f'\\nReached {total_tokens/1e9:.1f}B!')\n",
                "                break\n",
                "            if os.path.exists('STOP_TRAINING'):\n",
                "                print('Stop signal received')\n",
                "                break\n",
                "            \n",
                "            with acc.accumulate(model):\n",
                "                with acc.autocast():\n",
                "                    out = model(ids, attention_mask=mask, labels=ids)\n",
                "                    loss = get_loss(out)\n",
                "                if loss is not None:\n",
                "                    acc.backward(loss)\n",
                "                    for o in opts:\n",
                "                        o.step()\n",
                "                        o.zero_grad()\n",
                "                    for s in scheds:\n",
                "                        s.step()\n",
                "            \n",
                "            step += 1\n",
                "            total_tokens += tpb\n",
                "            \n",
                "            if step % 10 == 0 and is_main:\n",
                "                lr = scheds[0].get_last_lr()[0] if scheds else 0\n",
                "                lv = loss.item() if loss is not None else 0\n",
                "                buf = sum(s.buffer_status() for s in ds.streams.values()) / 1e6 if ds.streams else 0\n",
                "                gs = get_gpu_stats()\n",
                "                wandb.log({'loss': lv, 'step': step, 'lr': lr, 'tokens_B': total_tokens/1e9, 'buffer_M': buf, **gs})\n",
                "                pbar.set_postfix({'loss': f'{lv:.4f}', 'gpu': f\"{gs.get('gpu_total_gb', 0):.1f}GB\"})\n",
                "                pbar.update(tpb * 10)\n",
                "            \n",
                "            if step % CONFIG['save_steps'] == 0 and is_main:\n",
                "                acc.wait_for_everyone()\n",
                "                ckpt = Path(CONFIG['checkpoint_dir']) / f'ckpt-{step}'\n",
                "                ckpt.mkdir(exist_ok=True, parents=True)\n",
                "                acc.unwrap_model(model).save_srde_weights(str(ckpt / 'weights.pt'))\n",
                "                print(f'\\nSaved: {ckpt}')\n",
                "    \n",
                "    except KeyboardInterrupt:\n",
                "        print('\\nInterrupted')\n",
                "    finally:\n",
                "        ds.stop_streams()\n",
                "        if is_main and step > 0:\n",
                "            acc.wait_for_everyone()\n",
                "            final = Path(CONFIG['checkpoint_dir']) / f'final-{int(total_tokens/1e9)}B'\n",
                "            final.mkdir(exist_ok=True, parents=True)\n",
                "            acc.unwrap_model(model).save_srde_weights(str(final / 'weights.pt'))\n",
                "            wandb.finish()\n",
                "        cleanup()\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    main()\n",
                "'''\n",
                "\n",
                "with open('train.py', 'w') as f:\n",
                "    f.write(TRAIN_SCRIPT)\n",
                "print(\"âœ… train.py written (batch_size=8)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === LAUNCH TRAINING ===\n",
                "\n",
                "import subprocess\n",
                "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)\n",
                "NUM_GPUS = len([l for l in result.stdout.strip().split('\\n') if l.startswith('GPU')])\n",
                "\n",
                "print(f\"ðŸš€ Launching on {NUM_GPUS} GPUs (batch_size=8)...\")\n",
                "!accelerate launch --num_processes={NUM_GPUS} --mixed_precision=bf16 train.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === STOP TRAINING (uncomment and run when you want to stop) ===\n",
                "# !touch STOP_TRAINING\n",
                "# print(\"Stop signal sent.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}