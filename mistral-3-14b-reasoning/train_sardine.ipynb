{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SaRDinE Training\n",
                "\n",
                "**Fully self-contained notebook**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === API KEYS ===\n",
                "import os\n",
                "os.environ['WANDB_API_KEY'] = 'abe32d5463fb2265eaea4563a571c07e5a39b7b6'\n",
                "os.environ['HF_TOKEN'] = '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'\n",
                "\n",
                "!pip install -q transformers==5.0.0rc1 torch datasets accelerate wandb huggingface_hub bitsandbytes tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "if not os.path.exists('srde.py'):\n",
                "    print(\"Cloning SRDE repo...\")\n",
                "    !git clone https://github.com/MinimaML/srde-mistral\n",
                "    %cd srde-mistral/mistral-3-14b-reasoning\n",
                "else:\n",
                "    print(\"SRDE found.\")\n",
                "\n",
                "# Patch srde.py for multi-GPU and aux loss\n",
                "print(\"Applying patches...\")\n",
                "with open('srde.py', 'r') as f:\n",
                "    content = f.read()\n",
                "\n",
                "# ===== PATCH 1: Multi-GPU lm_head device fix =====\n",
                "old_device = '''                # Get device from language model\n",
                "                device = next(language_model.parameters()).device\n",
                "                \n",
                "                lm_head = nn.Linear(hidden_size, vocab_size, bias=False, device=device, dtype=torch_dtype)'''\n",
                "\n",
                "new_device = '''                # Get target device from device_map\n",
                "                if isinstance(device_map, dict) and '' in device_map:\n",
                "                    target_device = f\"cuda:{device_map['']}\" if isinstance(device_map[''], int) else device_map['']\n",
                "                elif isinstance(device_map, str) and device_map.startswith('cuda'):\n",
                "                    target_device = device_map\n",
                "                else:\n",
                "                    target_device = next(language_model.parameters()).device\n",
                "                \n",
                "                print(f\"[SRDE] Creating lm_head on: {target_device}\")\n",
                "                lm_head = nn.Linear(hidden_size, vocab_size, bias=False, device=target_device, dtype=torch_dtype)'''\n",
                "\n",
                "if old_device in content:\n",
                "    content = content.replace(old_device, new_device)\n",
                "    content = content.replace(\n",
                "        'lm_head.weight.copy_(language_model.embed_tokens.weight)',\n",
                "        'lm_head.weight.copy_(language_model.embed_tokens.weight.to(target_device))'\n",
                "    )\n",
                "    content = content.replace(\n",
                "        'print(f\"[SRDE] Initialized lm_head from embed_tokens (trainable copy, device={device})\")',\n",
                "        'print(f\"[SRDE] lm_head initialized on {target_device}\")'\n",
                "    )\n",
                "    print(\"  âœ… Multi-GPU lm_head fix\")\n",
                "\n",
                "# ===== PATCH 2: Fix aux loss computation (root cause of 'Error computing aux loss') =====\n",
                "old_aux = '''            router_probs = F.softmax(router_logits, dim=-1)\n",
                "            avg_probs = router_probs.mean(dim=list(range(router_probs.dim() - 1)))'''\n",
                "\n",
                "new_aux = '''            # router_logits shape: [batch, seq, num_experts] or [batch*seq, num_experts]\n",
                "            router_probs = F.softmax(router_logits, dim=-1)\n",
                "            \n",
                "            # Flatten all dims except last (expert dim) and compute mean\n",
                "            if router_probs.dim() > 1:\n",
                "                flat_probs = router_probs.view(-1, router_probs.size(-1))\n",
                "                avg_probs = flat_probs.mean(dim=0)\n",
                "            else:\n",
                "                avg_probs = router_probs'''\n",
                "\n",
                "if old_aux in content:\n",
                "    content = content.replace(old_aux, new_aux)\n",
                "    print(\"  âœ… Aux loss computation fix\")\n",
                "\n",
                "# ===== PATCH 3: Spam fix =====\n",
                "old_spam = '''        except Exception as e:\n",
                "            logger.warning(f\"Error computing aux loss: {e}\")\n",
                "            return torch.tensor(0.0, device=router_logits.device)'''\n",
                "\n",
                "new_spam = '''        except Exception as e:\n",
                "            if not hasattr(self, '_aux_warned'):\n",
                "                logger.warning(f\"Aux loss skipped: {type(e).__name__}: {e}\")\n",
                "                self._aux_warned = True\n",
                "            return torch.tensor(0.0, device=router_logits.device if router_logits is not None else 'cpu', requires_grad=True)'''\n",
                "\n",
                "if old_spam in content:\n",
                "    content = content.replace(old_spam, new_spam)\n",
                "    print(\"  âœ… Aux loss spam fix\")\n",
                "\n",
                "# ===== PATCH 4: Null check =====\n",
                "old_null = '''    def _compute_aux_loss(self, router_logits: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Compute load balancing auxiliary loss.\"\"\"\n",
                "        try:'''\n",
                "\n",
                "new_null = '''    def _compute_aux_loss(self, router_logits: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Compute load balancing auxiliary loss.\"\"\"\n",
                "        if router_logits is None:\n",
                "            return torch.tensor(0.0, device='cuda' if torch.cuda.is_available() else 'cpu', requires_grad=True)\n",
                "        try:'''\n",
                "\n",
                "if old_null in content and 'if router_logits is None:' not in content:\n",
                "    content = content.replace(old_null, new_null)\n",
                "    print(\"  âœ… Null check added\")\n",
                "\n",
                "with open('srde.py', 'w') as f:\n",
                "    f.write(content)\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi -L\n",
                "import subprocess\n",
                "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)\n",
                "NUM_GPUS = len([l for l in result.stdout.strip().split('\\n') if l.startswith('GPU')])\n",
                "print(f\"\\nDetected {NUM_GPUS} GPUs\")\n",
                "!rm -f STOP_TRAINING"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === WRITE TRAINING SCRIPT ===\n",
                "\n",
                "TRAIN_SCRIPT = r'''\n",
                "#!/usr/bin/env python3\n",
                "\"\"\"SaRDinE Multi-GPU Training\"\"\"\n",
                "import os, gc, random, time\n",
                "from pathlib import Path\n",
                "\n",
                "import torch\n",
                "import torch.distributed as dist\n",
                "import wandb\n",
                "import bitsandbytes as bnb\n",
                "from datasets import load_dataset, interleave_datasets\n",
                "from transformers import AutoTokenizer, get_cosine_schedule_with_warmup\n",
                "from torch.utils.data import DataLoader\n",
                "from tqdm.auto import tqdm\n",
                "from huggingface_hub import login\n",
                "from accelerate import Accelerator\n",
                "\n",
                "from config import SRDEConfig\n",
                "from srde import create_srde_model\n",
                "from muon import Muon\n",
                "\n",
                "if os.environ.get('HF_TOKEN'):\n",
                "    login(token=os.environ['HF_TOKEN'])\n",
                "\n",
                "MODEL_NAME = 'mistralai/Ministral-3-14B-Reasoning-2512'\n",
                "CONFIG = {\n",
                "    'wandb_project': 'sardine-collab', 'model_name': MODEL_NAME, 'max_length': 2048,\n",
                "    'target_tokens': 20_000_000_000, 'max_steps': 1000000,\n",
                "    'lr': 1e-4, 'muon_lr': 0.02, 'warmup_steps': 1000, 'save_steps': 5000,\n",
                "    'batch_size': 8, 'grad_accum': 4, 'checkpoint_dir': './checkpoints',\n",
                "}\n",
                "\n",
                "DATA_SOURCES = [\n",
                "    ('nvidia/OpenMathInstruct-2', None, 0.25),\n",
                "    ('meta-math/MetaMathQA', None, 0.15),\n",
                "    ('ise-uiuc/Magicoder-OSS-Instruct-75K', None, 0.20),\n",
                "    ('Rowan/hellaswag', None, 0.15),\n",
                "    ('deepmind/aqua_rat', 'raw', 0.10),\n",
                "    ('winogrande', 'winogrande_xl', 0.15),\n",
                "]\n",
                "\n",
                "def format_sample(sample):\n",
                "    for key in ['text', 'content', 'problem', 'question', 'sentence', 'ctx']:\n",
                "        if key in sample and sample[key]:\n",
                "            text = str(sample[key])\n",
                "            for ans_key in ['answer', 'solution', 'response', 'generated_solution', 'rationale']:\n",
                "                if ans_key in sample and sample[ans_key]:\n",
                "                    text += f\"\\n{sample[ans_key]}\"\n",
                "                    break\n",
                "            return {'text': text}\n",
                "    return {'text': str(sample)[:2000]}\n",
                "\n",
                "def get_loss(out):\n",
                "    return out.get('loss') if isinstance(out, dict) else getattr(out, 'loss', None)\n",
                "\n",
                "def get_params_for_muon(p):\n",
                "    return [x for x in p if x.ndim >= 2], [x for x in p if x.ndim < 2]\n",
                "\n",
                "def cleanup():\n",
                "    gc.collect()\n",
                "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
                "    if dist.is_initialized(): dist.destroy_process_group()\n",
                "\n",
                "def main():\n",
                "    local_rank = int(os.environ.get('LOCAL_RANK', 0))\n",
                "    world_size = int(os.environ.get('WORLD_SIZE', 1))\n",
                "    if torch.cuda.is_available(): torch.cuda.set_device(local_rank)\n",
                "    \n",
                "    print(f'[R{local_rank}/{world_size}] Starting')\n",
                "    \n",
                "    acc = Accelerator(mixed_precision='bf16', gradient_accumulation_steps=CONFIG['grad_accum'])\n",
                "    is_main = acc.is_main_process\n",
                "    \n",
                "    if is_main:\n",
                "        wandb.init(project=CONFIG['wandb_project'], name=f'sardine-{world_size}gpu', config=CONFIG)\n",
                "    \n",
                "    tok = AutoTokenizer.from_pretrained(CONFIG['model_name'], trust_remote_code=True)\n",
                "    tok.pad_token = tok.eos_token\n",
                "    \n",
                "    print(f'[R{local_rank}] Loading datasets...')\n",
                "    datasets = []\n",
                "    probs = []\n",
                "    for name, config, prob in DATA_SOURCES:\n",
                "        try:\n",
                "            ds = load_dataset(name, config, split='train', streaming=True, token=True)\n",
                "            ds = ds.map(format_sample, remove_columns=ds.column_names)\n",
                "            datasets.append(ds)\n",
                "            probs.append(prob)\n",
                "            if local_rank == 0: print(f'  âœ“ {name}')\n",
                "        except Exception as e:\n",
                "            if local_rank == 0: print(f'  âœ— {name}: {e}')\n",
                "    \n",
                "    if not datasets:\n",
                "        print('No datasets!')\n",
                "        return\n",
                "    \n",
                "    total = sum(probs)\n",
                "    probs = [p/total for p in probs]\n",
                "    combined = interleave_datasets(datasets, probabilities=probs, stopping_strategy='all_exhausted')\n",
                "    \n",
                "    def collate(batch):\n",
                "        texts = [b['text'][:8192] for b in batch]\n",
                "        e = tok(texts, truncation=True, max_length=CONFIG['max_length'], padding='max_length', return_tensors='pt')\n",
                "        return {'input_ids': e['input_ids'], 'attention_mask': e['attention_mask']}\n",
                "    \n",
                "    dl = DataLoader(combined, batch_size=CONFIG['batch_size'], collate_fn=collate)\n",
                "    \n",
                "    print(f'[R{local_rank}] Loading model...')\n",
                "    model = create_srde_model(CONFIG['model_name'], torch_dtype=torch.bfloat16, trust_remote_code=True, device_map={'': local_rank})\n",
                "    \n",
                "    tc = 0\n",
                "    for n, p in model.named_parameters():\n",
                "        if 'srde_layers' in n or 'expert' in n or 'router' in n or 'vocabulary' in n:\n",
                "            p.requires_grad = True\n",
                "            tc += p.numel()\n",
                "        else:\n",
                "            p.requires_grad = False\n",
                "    print(f'[R{local_rank}] Trainable: {tc/1e6:.1f}M')\n",
                "    \n",
                "    if hasattr(model, 'base_model') and hasattr(model.base_model, 'gradient_checkpointing_enable'):\n",
                "        model.base_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})\n",
                "    \n",
                "    all_p = [p for p in model.parameters() if p.requires_grad]\n",
                "    muon_p, adam_p = get_params_for_muon(all_p)\n",
                "    \n",
                "    opts, scheds = [], []\n",
                "    if muon_p:\n",
                "        o = Muon(muon_p, lr=CONFIG['muon_lr'], momentum=0.95)\n",
                "        opts.append(o)\n",
                "        scheds.append(get_cosine_schedule_with_warmup(o, CONFIG['warmup_steps'], CONFIG['max_steps']))\n",
                "    if adam_p:\n",
                "        o = bnb.optim.AdamW8bit(adam_p, lr=CONFIG['lr'])\n",
                "        opts.append(o)\n",
                "        scheds.append(get_cosine_schedule_with_warmup(o, CONFIG['warmup_steps'], CONFIG['max_steps']))\n",
                "    \n",
                "    model, dl, *rest = acc.prepare(model, dl, *opts, *scheds)\n",
                "    n = len(opts)\n",
                "    opts, scheds = list(rest[:n]), list(rest[n:])\n",
                "    \n",
                "    print(f'[R{local_rank}] Ready!')\n",
                "    \n",
                "    Path(CONFIG['checkpoint_dir']).mkdir(exist_ok=True, parents=True)\n",
                "    step, total_tokens = 0, 0\n",
                "    tpb = CONFIG['batch_size'] * CONFIG['max_length'] * world_size\n",
                "    pbar = tqdm(total=CONFIG['target_tokens'], unit='tok', unit_scale=True, disable=not is_main)\n",
                "    model.train()\n",
                "    \n",
                "    try:\n",
                "        for batch in dl:\n",
                "            if os.path.exists('STOP_TRAINING') or total_tokens >= CONFIG['target_tokens']:\n",
                "                break\n",
                "            \n",
                "            ids = batch['input_ids']\n",
                "            mask = batch['attention_mask']\n",
                "            \n",
                "            with acc.accumulate(model):\n",
                "                with acc.autocast():\n",
                "                    out = model(ids, attention_mask=mask, labels=ids)\n",
                "                    loss = get_loss(out)\n",
                "                if loss is not None:\n",
                "                    acc.backward(loss)\n",
                "                    for o in opts: o.step(); o.zero_grad()\n",
                "                    for s in scheds: s.step()\n",
                "            \n",
                "            step += 1\n",
                "            total_tokens += tpb\n",
                "            \n",
                "            if step % 10 == 0 and is_main:\n",
                "                lv = loss.item() if loss else 0\n",
                "                mem = torch.cuda.memory_allocated(local_rank) / 1e9\n",
                "                wandb.log({'loss': lv, 'step': step, 'tokens_B': total_tokens/1e9, 'gpu_gb': mem})\n",
                "                pbar.set_postfix({'loss': f'{lv:.4f}', 'mem': f'{mem:.1f}GB'})\n",
                "                pbar.update(tpb * 10)\n",
                "            \n",
                "            if step % CONFIG['save_steps'] == 0 and is_main:\n",
                "                acc.wait_for_everyone()\n",
                "                ckpt = Path(CONFIG['checkpoint_dir']) / f'ckpt-{step}'\n",
                "                ckpt.mkdir(exist_ok=True, parents=True)\n",
                "                acc.unwrap_model(model).save_srde_weights(str(ckpt / 'weights.pt'))\n",
                "                print(f'\\nSaved {ckpt}')\n",
                "    except Exception as e:\n",
                "        print(f'[R{local_rank}] Error: {e}')\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "    finally:\n",
                "        if is_main and step > 0:\n",
                "            acc.wait_for_everyone()\n",
                "            final = Path(CONFIG['checkpoint_dir']) / f'final-{int(total_tokens/1e9)}B'\n",
                "            final.mkdir(exist_ok=True, parents=True)\n",
                "            acc.unwrap_model(model).save_srde_weights(str(final / 'weights.pt'))\n",
                "            wandb.finish()\n",
                "        cleanup()\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    main()\n",
                "'''\n",
                "\n",
                "with open('train.py', 'w') as f:\n",
                "    f.write(TRAIN_SCRIPT)\n",
                "print(\"âœ… train.py written (batch_size=8)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === LAUNCH TRAINING ===\n",
                "import subprocess\n",
                "result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True)\n",
                "NUM_GPUS = len([l for l in result.stdout.strip().split('\\n') if l.startswith('GPU')])\n",
                "\n",
                "print(f\"ðŸš€ Launching on {NUM_GPUS} GPUs...\")\n",
                "!accelerate launch --num_processes={NUM_GPUS} --mixed_precision=bf16 train.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === STOP TRAINING ===\n",
                "# !touch STOP_TRAINING"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}