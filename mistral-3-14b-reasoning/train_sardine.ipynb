{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŸ SaRDinE Training Notebook\n",
                "\n",
                "**Sparse Routed Delta Experts on Mistral-14B-Reasoning**\n",
                "\n",
                "This notebook trains SRDE with:\n",
                "- W&B logging\n",
                "- Auto-upload to HuggingFace\n",
                "- Ablation support"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q torch transformers datasets accelerate wandb huggingface_hub bitsandbytes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone the SRDE repo\n",
                "!git clone https://github.com/MinimaML/srde-mistral\n",
                "%cd srde-mistral/mistral-3-14b-reasoning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Login to W&B (paste your API key when prompted)\n",
                "import wandb\n",
                "wandb.login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Login to HuggingFace (for auto-upload)\n",
                "from huggingface_hub import login\n",
                "login()  # Paste your HF token when prompted"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training config - edit these!\n",
                "CONFIG = {\n",
                "    # W&B\n",
                "    \"wandb_project\": \"sardine-collab\",\n",
                "    \"wandb_run_name\": \"sardine-b200-run\",\n",
                "    \n",
                "    # HuggingFace upload\n",
                "    \"hf_repo\": \"MinimaML/SaRDinE-14B8x4P\",\n",
                "    \n",
                "    # Training\n",
                "    \"num_epochs\": 3,\n",
                "    \"max_steps\": 100000,\n",
                "    \"batch_size\": 4,  # Increase for B200!\n",
                "    \"gradient_accumulation_steps\": 4,\n",
                "    \"learning_rate\": 1e-4,\n",
                "    \"save_steps\": 2500,\n",
                "    \n",
                "    # SRDE\n",
                "    \"num_experts\": 8,\n",
                "    \"target_sparsity\": 0.01,\n",
                "    \n",
                "    # Ablation (set to None for normal run)\n",
                "    \"ablation_name\": None,  # e.g., \"4_experts\", \"no_router\"\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoTokenizer\n",
                "from srde import create_srde_model\n",
                "from config import SRDEConfig\n",
                "\n",
                "print(\"Loading model...\")\n",
                "\n",
                "# Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(\n",
                "    \"mistralai/Ministral-3-14B-Reasoning-2512\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# SRDE Model\n",
                "model = create_srde_model(\n",
                "    model_name=\"mistralai/Ministral-3-14B-Reasoning-2512\",\n",
                "    torch_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "print(f\"Trainable params: {model.trainable_param_count() / 1e9:.2f}B\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Load dataset\n",
                "dataset = load_dataset(\"MinimaML/srde-reasoning-mix\", split=\"train\")\n",
                "\n",
                "def tokenize(examples):\n",
                "    return tokenizer(\n",
                "        examples[\"text\"],\n",
                "        truncation=True,\n",
                "        max_length=2048,\n",
                "        padding=\"max_length\"\n",
                "    )\n",
                "\n",
                "dataset = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names)\n",
                "dataset.set_format(\"torch\")\n",
                "\n",
                "dataloader = DataLoader(\n",
                "    dataset,\n",
                "    batch_size=CONFIG[\"batch_size\"],\n",
                "    shuffle=True,\n",
                "    num_workers=4\n",
                ")\n",
                "\n",
                "print(f\"Dataset size: {len(dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Setup Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datetime import datetime\n",
                "\n",
                "# W&B init\n",
                "run_name = CONFIG[\"wandb_run_name\"] or f\"sardine-{datetime.now().strftime('%m%d-%H%M')}\"\n",
                "if CONFIG[\"ablation_name\"]:\n",
                "    run_name = f\"ablation-{CONFIG['ablation_name']}\"\n",
                "\n",
                "wandb.init(\n",
                "    project=CONFIG[\"wandb_project\"],\n",
                "    name=run_name,\n",
                "    config=CONFIG\n",
                ")\n",
                "\n",
                "# Optimizer\n",
                "optimizer = torch.optim.AdamW(\n",
                "    model.get_trainable_params(),\n",
                "    lr=CONFIG[\"learning_rate\"],\n",
                "    weight_decay=0.01\n",
                ")\n",
                "\n",
                "print(\"Training setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.auto import tqdm\n",
                "from pathlib import Path\n",
                "\n",
                "device = \"cuda\"\n",
                "model.train()\n",
                "global_step = 0\n",
                "best_loss = float('inf')\n",
                "\n",
                "output_dir = Path(\"./checkpoints\")\n",
                "output_dir.mkdir(exist_ok=True)\n",
                "\n",
                "for epoch in range(CONFIG[\"num_epochs\"]):\n",
                "    print(f\"\\n=== Epoch {epoch} ===\")\n",
                "    \n",
                "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}\")\n",
                "    \n",
                "    for batch_idx, batch in enumerate(pbar):\n",
                "        if global_step >= CONFIG[\"max_steps\"]:\n",
                "            break\n",
                "            \n",
                "        input_ids = batch[\"input_ids\"].to(device)\n",
                "        attention_mask = batch[\"attention_mask\"].to(device)\n",
                "        \n",
                "        # Forward\n",
                "        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
                "            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
                "            loss = outputs.loss / CONFIG[\"gradient_accumulation_steps\"]\n",
                "        \n",
                "        # Backward\n",
                "        loss.backward()\n",
                "        \n",
                "        # Update\n",
                "        if (batch_idx + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
                "            optimizer.step()\n",
                "            optimizer.zero_grad()\n",
                "            global_step += 1\n",
                "            \n",
                "            current_loss = loss.item() * CONFIG[\"gradient_accumulation_steps\"]\n",
                "            \n",
                "            # Log to W&B\n",
                "            if global_step % 10 == 0:\n",
                "                wandb.log({\"loss\": current_loss, \"step\": global_step})\n",
                "                pbar.set_postfix({\"loss\": f\"{current_loss:.4f}\", \"step\": global_step})\n",
                "            \n",
                "            # Save checkpoint\n",
                "            if global_step % CONFIG[\"save_steps\"] == 0:\n",
                "                save_path = output_dir / f\"checkpoint-{global_step}\"\n",
                "                save_path.mkdir(exist_ok=True)\n",
                "                model.save_srde_weights(str(save_path / \"srde_weights.pt\"))\n",
                "                print(f\"\\nSaved checkpoint to {save_path}\")\n",
                "                \n",
                "                if current_loss < best_loss:\n",
                "                    best_loss = current_loss\n",
                "                    best_path = output_dir / \"checkpoint-best\"\n",
                "                    best_path.mkdir(exist_ok=True)\n",
                "                    model.save_srde_weights(str(best_path / \"srde_weights.pt\"))\n",
                "\n",
                "print(f\"\\nTraining complete! Final step: {global_step}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Upload to HuggingFace"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import HfApi\n",
                "\n",
                "api = HfApi()\n",
                "\n",
                "# Upload best checkpoint\n",
                "api.upload_folder(\n",
                "    folder_path=\"./checkpoints/checkpoint-best\",\n",
                "    repo_id=CONFIG[\"hf_repo\"],\n",
                "    commit_message=f\"Training complete - {global_step} steps - loss {best_loss:.4f}\"\n",
                ")\n",
                "\n",
                "print(f\"Uploaded to: https://huggingface.co/{CONFIG['hf_repo']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Finish W&B\n",
                "wandb.finish()\n",
                "print(\"Done! ðŸŸ\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}